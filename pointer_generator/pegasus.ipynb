{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset gigaword (/tmp/xdg-cache/huggingface/datasets/gigaword/default/1.2.0/c518c578e42a6afe842b09e979ee2907ea42a12b57ba992fae9e9d7347825245)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import math\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gigaword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "src_text = dataset['train']['document'][0:1000]\n",
    "target_text = dataset['train']['summary'][0:1000]\n",
    "model_name = 'google/pegasus-gigaword'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_data = tokenizer.prepare_seq2seq_batch(src_text, target_text, return_tensors=\"pt\", truncation=\"only_first\", padding=\"longest\", max_length=64)\n",
    "\n",
    "input_ids_train = train_data['input_ids']\n",
    "attention_masks_train = train_data['attention_mask']\n",
    "labels_train = train_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, PegasusConfig, modeling_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PegasusConfig.from_pretrained(model_name, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusGuidedCopyBack were not initialized from the model checkpoint at google/pegasus-gigaword and are newly initialized: ['outdegree_score_w', 'indegree_score_w', 'p_gen_w.weight', 'p_gen_w.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pega_copyback_model = PegasusForConditionalGeneration.from_pretrained(model_name, config=config).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "\n",
    "batch_size = 2\n",
    "dataloader_train = DataLoader(dataset_train,sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "\n",
    "#freezing the parameters\n",
    "# for param in pega_copyback_model.model.parameters():\n",
    "#     param.requires_grad = False\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, pega_copyback_model.parameters()),lr=5e-5)\n",
    "                  \n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfbd0c88147469a973601814f9589bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esen/.local/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 1\n",
      "\r",
      "Training loss: 5.9626273908615115\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 2', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 2\n",
      "\r",
      "Training loss: 0.8384230832308531\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 3', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 3\n",
      "\r",
      "Training loss: 0.6054406896829605\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 4', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 4\n",
      "\r",
      "Training loss: 0.4688126783259213\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 5', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 5\n",
      "\r",
      "Training loss: 0.34567064413055776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 6', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 6\n",
      "\r",
      "Training loss: 0.26391870155371727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a5ddb2c23d4a93b524b7de18af8478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 7', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-08387f1d2b40>\", line 38, in <module>\n",
      "    optimizer.step()\n",
      "  File \"/home/esen/.local/lib/python3.7/site-packages/transformers/optimization.py\", line 304, in step\n",
      "    p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/esen/.local/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/esen/.local/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/opt/conda/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/esen/.local/lib/python3.7/site-packages/tensorflow_core/contrib/__init__.py\", line 43, in <module>\n",
      "    from tensorflow.contrib import cudnn_rnn\n",
      "  File \"/home/esen/.local/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/__init__.py\", line 38, in <module>\n",
      "    from tensorflow.contrib.cudnn_rnn.python.layers import *\n",
      "  File \"/home/esen/.local/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/__init__.py\", line 38, in <module>\n",
      "    from tensorflow.contrib.cudnn_rnn.python.layers import *\n",
      "  File \"/home/esen/.local/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/layers/__init__.py\", line 23, in <module>\n",
      "    from tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import *\n",
      "  File \"/home/esen/.local/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 20, in <module>\n",
      "    from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\n",
      "  File \"/home/esen/.local/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 21, in <module>\n",
      "    from tensorflow.contrib.checkpoint.python import split_dependency\n",
      "ImportError: cannot import name 'split_dependency' from 'tensorflow.contrib.checkpoint.python' (unknown location)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)  \n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    pega_copyback_model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for b in progress_bar:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        b = tuple(x.to(torch_device) for x in b)\n",
    "        \n",
    "        inputs = {'input_ids':      b[0],\n",
    "                  'attention_mask': b[1],\n",
    "                  'labels':         b[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = pega_copyback_model(**inputs)\n",
    "#         vocab_size =  outputs['logits'].shape[2]\n",
    "    \n",
    "        loss = outputs[0]#criterion(outputs.view(-1,vocab_size),b[2].view(-1))\n",
    "\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "#         torch.nn.utils.clip_grad_norm_(pega_copyback_model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(b))})\n",
    "         \n",
    "        \n",
    "    torch.save(pega_copyback_model.state_dict(), f'data/finetuned_pega_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pega_copyback_model.eval()\n",
    "\n",
    "tgt_text = tokenizer.batch_decode(pega_copyback_model.generate(train_data['input_ids'][[0],:].to(torch_device)))#, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['australian current current account current account current account narrows #.## bln dlrs in june quarter on commodity price rise narrows to #.unk_3']\n"
     ]
    }
   ],
   "source": [
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': AggregateScore(low=Score(precision=0.1554928400944054, recall=0.4795815746753247, fmeasure=0.23136775513804625), mid=Score(precision=0.17402431231662066, recall=0.5282606421356422, fmeasure=0.2569692341725045), high=Score(precision=0.19291390263704156, recall=0.579778896103896, fmeasure=0.2824507151708098)), 'rouge2': AggregateScore(low=Score(precision=0.06152143285722438, recall=0.2030489538239538, fmeasure=0.09302674511225241), mid=Score(precision=0.07464696147510039, recall=0.24736994949494936, fmeasure=0.11211079923104261), high=Score(precision=0.0904801216513209, recall=0.29384617604617597, fmeasure=0.13413271281551845)), 'rougeL': AggregateScore(low=Score(precision=0.14158101591581065, recall=0.43408622835497856, fmeasure=0.21143435889558784), mid=Score(precision=0.1583793080593422, recall=0.48157323232323257, fmeasure=0.23368659118166588), high=Score(precision=0.17625338100710924, recall=0.5238439213564217, fmeasure=0.25546915132337167)), 'rougeLsum': AggregateScore(low=Score(precision=0.14182272774856935, recall=0.4324151695526696, fmeasure=0.2110467875827964), mid=Score(precision=0.1586512373080322, recall=0.48265043290043297, fmeasure=0.2340135168433362), high=Score(precision=0.17774924731361053, recall=0.529331556637807, fmeasure=0.2584433259230718))}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "pega_copyback_model.eval()\n",
    "\n",
    "\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "src_text = dataset['validation']['document'][0:100]\n",
    "target_text = dataset['validation']['summary'][0:100]\n",
    "\n",
    "val_data = tokenizer.prepare_seq2seq_batch(src_text, target_text, return_tensors=\"pt\", truncation=\"only_first\", padding=\"longest\", max_length=64)\n",
    "\n",
    "input_ids_val = val_data['input_ids']\n",
    "attention_masks_val = val_data['attention_mask']\n",
    "labels_val = val_data['labels']\n",
    "\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "batch_size = 1\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size)\n",
    "\n",
    "\n",
    "shouldPrint = 0\n",
    "toPrint = 4\n",
    "with torch.no_grad():\n",
    "    for b in dataloader_val:        \n",
    "        b = tuple(x.to(torch_device) for x in b)\n",
    "        \n",
    "        inputs = {'input_ids':      b[0],\n",
    "                  'attention_mask': b[1],\n",
    "                  'labels':         b[2],\n",
    "                 }       \n",
    "        gen = tokenizer.batch_decode(pega_copyback_model.generate(inputs['input_ids']), skip_special_tokens=True)\n",
    "        if shouldPrint < toPrint:\n",
    "            print(\"Source Sentence:\", src_text[shouldPrint])\n",
    "            print(\"Sample Sentence:\", gen[0])\n",
    "            shouldPrint += 1 \n",
    "        ref = tokenizer.batch_decode(inputs['labels'])\n",
    "        metric.add_batch(predictions=gen, references=ref)\n",
    "\n",
    "\n",
    "print(\"@@@@@@@@@@@@@@@@@@@ Rogue @@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "print()        \n",
    "\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
