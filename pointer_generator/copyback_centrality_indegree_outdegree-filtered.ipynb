{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset gigaword (/tmp/xdg-cache/huggingface/datasets/gigaword/default/1.2.0/c518c578e42a6afe842b09e979ee2907ea42a12b57ba992fae9e9d7347825245)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import math\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gigaword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "entailed = pd.read_csv(\"../data/gigawordfiltered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947\n",
      "947\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "src_text = [text for i,text in enumerate(dataset['train']['document'][0:1057]) if entailed['index_keys'][i] == 1]\n",
    "target_text = [text for i,text in enumerate(dataset['train']['summary'][0:1057]) if entailed['index_keys'][i] == 1]\n",
    "print(len(src_text))\n",
    "print(len(target_text))\n",
    "\n",
    "model_name = 'google/pegasus-gigaword'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#model = PegasusForConditionalGeneration.from_pretrained(model_name,return_dict=True,output_attentions=True,output_hidden_states=True).to(torch_device)\n",
    "\n",
    "train_data = tokenizer.prepare_seq2seq_batch(src_text, target_text, return_tensors=\"pt\", truncation=\"only_first\", padding=\"longest\", max_length=64)\n",
    "\n",
    "input_ids_train = train_data['input_ids']\n",
    "attention_masks_train = train_data['attention_mask']\n",
    "labels_train = train_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, PegasusConfig, modeling_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PegasusGuidedCopyBack(PegasusForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super(PegasusGuidedCopyBack, self).__init__(config)\n",
    "        num_features = config.d_model\n",
    "        self.p_gen_w = nn.Linear(num_features*3,1)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.p_gen_w.bias = nn.Parameter(torch.ones(1))\n",
    "        self.model_size = num_features\n",
    "        self.outdegree_score_w = nn.Parameter(torch.ones(1) * 0.5)\n",
    "        self.indegree_score_w = nn.Parameter(torch.ones(1) * 0.5)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):  \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        out = super(PegasusGuidedCopyBack, self).forward(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            labels=labels,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        # score centrality addition\n",
    "        queries_matrix = self.model.decoder.layers[-1].encoder_attn.q_proj(out['decoder_hidden_states'][-2])\n",
    "        if 'encoder_hidden_states' in out:\n",
    "            keys_matrix = self.model.encoder.layers[-1].self_attn.k_proj(out['encoder_hidden_states'][-2])\n",
    "        else:\n",
    "            keys_matrix = self.model.encoder.layers[-1].self_attn.k_proj(self.encoder_second_hidden)\n",
    "        \n",
    "        if 'encoder_attentions' in out:\n",
    "            self_attn_graph = torch.sum(out['encoder_attentions'][-1],dim=1) # (batch_size x s_seq_len x s_seq_len)\n",
    "        else:\n",
    "            self_attn_graph = torch.sum(self.encoder_attentions, dim=1)\n",
    "        \n",
    "        \n",
    "        outdegree_score = torch.sum(self_attn_graph,dim=1).unsqueeze(-1) #(batch_size x s_seq_len x 1)\n",
    "        temp_matrix = self_attn_graph\n",
    "        transition_matrix = F.normalize(temp_matrix,p=1,dim=2)\n",
    "        indegree_score = torch.sum(transition_matrix,dim=1).unsqueeze(-1)\n",
    "        \n",
    "        temp = keys_matrix + self.outdegree_score_w * outdegree_score + self.indegree_score_w * indegree_score\n",
    "        attn = self.softmax(queries_matrix@(temp.transpose(2,1))/math.sqrt(self.model_size))\n",
    "\n",
    "        if labels is not None:\n",
    "            dec = self.model.get_input_embeddings()(labels) # (batch_size x t_seq_len x model_size)\n",
    "        else:\n",
    "            dec = self.model.get_input_embeddings()(decoder_input_ids[:,[-1]])\n",
    "        \n",
    "        hi = out['encoder_last_hidden_state'] # (batch_size x s_seq_len x model_size)      \n",
    "        st = out['decoder_hidden_states'][-1] # (batch_size x t_seq_len x model_size)\n",
    "        \n",
    "        p_gen = torch.sigmoid(self.p_gen_w(torch.cat((attn @ hi,st,dec),dim=-1))) # (batch_size x t_seq_len x 1)\n",
    "\n",
    "        v_dist = p_gen*out['logits'] # (batch_size x t_seq_len x vocab_size)\n",
    "        \n",
    "        attn_dists = (1-p_gen)*attn # (batch_size x t_seq_len x s_seq_len)\n",
    "        \n",
    "        if input_ids is not None:            \n",
    "            src_ids = input_ids.unsqueeze(1).repeat(1, attn_dists.size(1), 1)   # (batch_size x 1 x s_seq_len)  \n",
    "        else:\n",
    "            src_ids = self.input_ids.unsqueeze(1).repeat(1, attn_dists.size(1), 1)   # (batch_size x 1 x s_seq_len)      \n",
    "#         print(src_ids.shape)\n",
    "#         print(v_dist.shape)\n",
    "#         print(attn_dists.shape)\n",
    "#         print(attn_dists.size(1))\n",
    "        total_score = (self.outdegree_score_w * outdegree_score + self.indegree_score_w * indegree_score).squeeze(-1)\n",
    "        total_score = F.softmax(total_score,dim=-1)\n",
    "        #total_score = F.softmax(outdegree_score.squeeze(-1),dim=-1)\n",
    "        \n",
    "        enc_dec_attn = torch.sum(attn_dists,dim=1)\n",
    "\n",
    "        kl_divergence = F.kl_div(total_score,enc_dec_attn)\n",
    "        pred = v_dist.scatter_add(2, src_ids, attn_dists) #(batch_size x t_seq_len x vocab_size)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # TODO(SS): do we need to ignore pad tokens in labels?\n",
    "            masked_lm_loss = loss_fct(pred.view(-1, self.config.vocab_size), labels.view(-1)) + kl_divergence\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (pred,) + out[1:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "        \n",
    "        return modeling_outputs.Seq2SeqLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=pred,\n",
    "            past_key_values=out.past_key_values,\n",
    "            decoder_hidden_states=out.decoder_hidden_states,\n",
    "            decoder_attentions=out.decoder_attentions,\n",
    "            cross_attentions=out.cross_attentions,\n",
    "            encoder_last_hidden_state=out.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=out.encoder_hidden_states,\n",
    "            encoder_attentions=out.encoder_attentions,\n",
    "        )\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        max_length=None,\n",
    "        min_length=None,\n",
    "        do_sample=None,\n",
    "        early_stopping=None,\n",
    "        num_beams=None,\n",
    "        temperature=None,\n",
    "        top_k=None,\n",
    "        top_p=None,\n",
    "        repetition_penalty=None,\n",
    "        bad_words_ids=None,\n",
    "        bos_token_id=None,\n",
    "        pad_token_id=None,\n",
    "        eos_token_id=None,\n",
    "        length_penalty=None,\n",
    "        no_repeat_ngram_size=None,\n",
    "        num_return_sequences=None,\n",
    "        attention_mask=None,\n",
    "        decoder_start_token_id=None,\n",
    "        use_cache=None,\n",
    "        **model_specific_kwargs\n",
    "    ):\n",
    "        self.input_ids = input_ids\n",
    "        out = super(PegasusGuidedCopyBack, self).forward(input_ids)\n",
    "        self.encoder_second_hidden = out['encoder_hidden_states'][-2]\n",
    "        self.encoder_attentions = out['encoder_attentions'][-1]\n",
    "        \n",
    "        return super(PegasusGuidedCopyBack, self).generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            do_sample=do_sample,\n",
    "            early_stopping=early_stopping,\n",
    "            num_beams=num_beams,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            bos_token_id=bos_token_id,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            length_penalty=length_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_start_token_id=decoder_start_token_id,\n",
    "            use_cache=use_cache,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PegasusConfig.from_pretrained(model_name, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusGuidedCopyBack were not initialized from the model checkpoint at google/pegasus-gigaword and are newly initialized: ['outdegree_score_w', 'indegree_score_w', 'p_gen_w.weight', 'p_gen_w.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pega_copyback_model = PegasusGuidedCopyBack.from_pretrained(model_name, config=config).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "\n",
    "batch_size = 2\n",
    "dataloader_train = DataLoader(dataset_train,sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "\n",
    "#freezing the parameters\n",
    "# for param in pega_copyback_model.model.parameters():\n",
    "#     param.requires_grad = False\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, pega_copyback_model.parameters()),lr=5e-5)\n",
    "                  \n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef4feaa254f481da90a7f3bc5cdd1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=474.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esen/.local/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 1\n",
      "\r",
      "Training loss: 7.250252215168144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 2', max=474.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 2\n",
      "\r",
      "Training loss: 1.5866059822340806\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 3', max=474.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 3\n",
      "\r",
      "Training loss: 0.675090633144107\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 4', max=474.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 4\n",
      "\r",
      "Training loss: 0.519468003467417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 5', max=474.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 5\n",
      "\r",
      "Training loss: 0.398780288807419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)  \n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    pega_copyback_model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for b in progress_bar:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        b = tuple(x.to(torch_device) for x in b)\n",
    "        \n",
    "        inputs = {'input_ids':      b[0],\n",
    "                  'attention_mask': b[1],\n",
    "                  'labels':         b[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = pega_copyback_model(**inputs)\n",
    "#         vocab_size =  outputs['logits'].shape[2]\n",
    "    \n",
    "        loss = outputs[0]#criterion(outputs.view(-1,vocab_size),b[2].view(-1))\n",
    "\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "#         torch.nn.utils.clip_grad_norm_(pega_copyback_model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(b))})\n",
    "         \n",
    "        \n",
    "    torch.save(pega_copyback_model.state_dict(), f'data/finetuned_pega_in_outdegree_filtered_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pega_copyback_model.eval()\n",
    "\n",
    "tgt_text = tokenizer.batch_decode(pega_copyback_model.generate(train_data['input_ids'][[0],:].to(torch_device)))#, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['australian australian australian australian australian australian australian current account current account current account deficit narrows australian current account deficit narrows australian current account deficit narrows #.unk_3']\n"
     ]
    }
   ],
   "source": [
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': AggregateScore(low=Score(precision=0.07872079594325206, recall=0.24750711580086587, fmeasure=0.1179690066388292), mid=Score(precision=0.09252074497511822, recall=0.2872151875901876, fmeasure=0.1377107346346041), high=Score(precision=0.1072972561640747, recall=0.3338265422077921, fmeasure=0.15935673330602604)), 'rouge2': AggregateScore(low=Score(precision=0.026462486831683726, recall=0.08466808261183262, fmeasure=0.03994401548319826), mid=Score(precision=0.03646153870759598, recall=0.11838762626262628, fmeasure=0.05501554192773852), high=Score(precision=0.04759849511188942, recall=0.15862531565656562, fmeasure=0.07195194785219448)), 'rougeL': AggregateScore(low=Score(precision=0.07616191244615397, recall=0.2366564574314574, fmeasure=0.11371305347403333), mid=Score(precision=0.0893201354396172, recall=0.277267316017316, fmeasure=0.13299962625777167), high=Score(precision=0.1032196970907561, recall=0.3189087932900434, fmeasure=0.15322803518562703)), 'rougeLsum': AggregateScore(low=Score(precision=0.07631236149862702, recall=0.2383299242424242, fmeasure=0.11383841923249488), mid=Score(precision=0.0891239305614624, recall=0.2776338383838383, fmeasure=0.13265570482094075), high=Score(precision=0.1037887733010388, recall=0.3207160894660894, fmeasure=0.15357754387332914))}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "pega_copyback_model.eval()\n",
    "\n",
    "\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "src_text = dataset['validation']['document'][0:100]\n",
    "target_text = dataset['validation']['summary'][0:100]\n",
    "\n",
    "val_data = tokenizer.prepare_seq2seq_batch(src_text, target_text, return_tensors=\"pt\", truncation=\"only_first\", padding=\"longest\", max_length=64)\n",
    "\n",
    "input_ids_val = val_data['input_ids']\n",
    "attention_masks_val = val_data['attention_mask']\n",
    "labels_val = val_data['labels']\n",
    "\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "batch_size = 1\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size)\n",
    "\n",
    "\n",
    "shouldPrint = 0\n",
    "toPrint = 4\n",
    "with torch.no_grad():\n",
    "    for b in dataloader_val:        \n",
    "        b = tuple(x.to(torch_device) for x in b)\n",
    "        \n",
    "        inputs = {'input_ids':      b[0],\n",
    "                  'attention_mask': b[1],\n",
    "                  'labels':         b[2],\n",
    "                 }       \n",
    "        gen = tokenizer.batch_decode(pega_copyback_model.generate(inputs['input_ids']), skip_special_tokens=True)\n",
    "        if shouldPrint < toPrint:\n",
    "            print(\"Source Sentence:\", src_text[shouldPrint])\n",
    "            print(\"Sample Sentence:\", gen[0])\n",
    "            shouldPrint += 1 \n",
    "        ref = tokenizer.batch_decode(inputs['labels'])\n",
    "        metric.add_batch(predictions=gen, references=ref)\n",
    "\n",
    "print(\"@@@@@@@@@@@@@@@@@@@ Rogue @@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "print()\n",
    "        \n",
    "print(metric.compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
