{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset gigaword (/tmp/xdg-cache/huggingface/datasets/gigaword/default/1.2.0/c518c578e42a6afe842b09e979ee2907ea42a12b57ba992fae9e9d7347825245)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gigaword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "src_text = dataset['train']['document'][0:1000]\n",
    "target_text = dataset['train']['summary'][0:1000]\n",
    "model_name = 'google/pegasus-gigaword'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_data = tokenizer.prepare_seq2seq_batch(src_text, target_text, return_tensors=\"pt\", truncation=\"only_first\", padding=\"longest\", max_length=64)\n",
    "\n",
    "#tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = train_data['input_ids']\n",
    "attention_masks_train = train_data['attention_mask']\n",
    "labels_train = train_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[14937,  1034,   116,  ...,     0,     0,     0],\n",
      "        [  134,   583,   228,  ...,     0,     0,     0],\n",
      "        [73013,  2853,  2127,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  260,   117, 32078,  ...,     0,     0,     0],\n",
      "        [  154,   197,   110,  ...,     0,     0,     0],\n",
      "        [  126,  1034,   116,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[73013,   582,   728,  ...,     0,     0,     0],\n",
      "        [  134,   583,   228,  ...,     0,     0,     0],\n",
      "        [73013,  5446,   686,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 3834,  6010, 12752,  ...,     0,     0,     0],\n",
      "        [32577,   493, 23765,  ...,     0,     0,     0],\n",
      "        [94830,  1034,   116,  ...,     0,     0,     0]])}\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, PegasusConfig, modeling_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PegasusWithCopyBack(PegasusForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super(PegasusWithCopyBack, self).__init__(config)\n",
    "        num_features = config.d_model\n",
    "        self.p_gen_w = nn.Linear(num_features*3,1)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.p_gen_w.bias = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):  \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        out = super(PegasusWithCopyBack, self).forward(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            labels=labels,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        hi = out['encoder_last_hidden_state'] # (batch_size x s_seq_len x model_size)\n",
    "        at = torch.mean(out['cross_attentions'][-1], dim=1) # (batch_size x t_seq_len x s_seq_len)        \n",
    "        st = out['decoder_hidden_states'][-1] # (batch_size x t_seq_len x model_size)\n",
    "        \n",
    "        if labels is not None:\n",
    "            dec = self.model.get_input_embeddings()(labels) # (batch_size x t_seq_len x model_size)\n",
    "        else:\n",
    "#             print(decoder_input_ids.shape)\n",
    "            dec = self.model.get_input_embeddings()(decoder_input_ids[:,[-1]])\n",
    "\n",
    "#         print((at@hi).shape)\n",
    "#         print(st.shape)\n",
    "#         print(dec.shape)\n",
    "        \n",
    "        p_gen = torch.sigmoid(self.p_gen_w(torch.cat((at @ hi,st,dec),dim=-1))) # (batch_size x t_seq_len x 1)\n",
    "        attn_dists = (1-p_gen)*at # (batch_size x t_seq_len x s_seq_len)\n",
    "        v_dist = p_gen * out['logits'] # (batch_size x t_seq_len x vocab_size)\n",
    "        \n",
    "\n",
    "        if input_ids is not None:            \n",
    "            src_ids = input_ids.unsqueeze(1).repeat(1, attn_dists.size(1), 1)   # (batch_size x 1 x s_seq_len)  \n",
    "        else:\n",
    "            src_ids = self.input_ids.unsqueeze(1).repeat(1, attn_dists.size(1), 1)   # (batch_size x 1 x s_seq_len)\n",
    "                \n",
    "        pred = v_dist.scatter_add(2, src_ids, attn_dists) #(batch_size x t_seq_len x vocab_size)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # TODO(SS): do we need to ignore pad tokens in labels?\n",
    "            masked_lm_loss = loss_fct(pred.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (pred,) + out[1:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "        \n",
    "        return modeling_outputs.Seq2SeqLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=pred,\n",
    "            past_key_values=out.past_key_values,\n",
    "            decoder_hidden_states=out.decoder_hidden_states,\n",
    "            decoder_attentions=out.decoder_attentions,\n",
    "            cross_attentions=out.cross_attentions,\n",
    "            encoder_last_hidden_state=out.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=out.encoder_hidden_states,\n",
    "            encoder_attentions=out.encoder_attentions,\n",
    "        )\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        max_length=None,\n",
    "        min_length=None,\n",
    "        do_sample=None,\n",
    "        early_stopping=None,\n",
    "        num_beams=None,\n",
    "        temperature=None,\n",
    "        top_k=None,\n",
    "        top_p=None,\n",
    "        repetition_penalty=None,\n",
    "        bad_words_ids=None,\n",
    "        bos_token_id=None,\n",
    "        pad_token_id=None,\n",
    "        eos_token_id=None,\n",
    "        length_penalty=None,\n",
    "        no_repeat_ngram_size=None,\n",
    "        num_return_sequences=None,\n",
    "        attention_mask=None,\n",
    "        decoder_start_token_id=None,\n",
    "        use_cache=None,\n",
    "        **model_specific_kwargs\n",
    "    ):\n",
    "        self.input_ids = input_ids\n",
    "        return super(PegasusWithCopyBack, self).generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            do_sample=do_sample,\n",
    "            early_stopping=early_stopping,\n",
    "            num_beams=num_beams,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            bos_token_id=bos_token_id,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            length_penalty=length_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_start_token_id=decoder_start_token_id,\n",
    "            use_cache=use_cache,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PegasusConfig.from_pretrained(model_name, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusWithCopyBack were not initialized from the model checkpoint at google/pegasus-gigaword and are newly initialized: ['p_gen_w.weight', 'p_gen_w.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# pega_copyback_model = PegasusWithCopyBack(config, 1024).to(torch_device) \n",
    "pega_copyback_model = PegasusWithCopyBack.from_pretrained(model_name, config=config).to(torch_device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "\n",
    "batch_size = 2\n",
    "dataloader_train = DataLoader(dataset_train,sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "\n",
    "#freezing the parameters\n",
    "# for param in pega_copyback_model.model.parameters():\n",
    "#     param.requires_grad = False\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, pega_copyback_model.parameters()),lr=5e-5)\n",
    "                  \n",
    "epochs = 10\n",
    "# #scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "#                                             num_warmup_steps=0,\n",
    "#                                             num_training_steps=len(dataloader_train)*epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08de6d8b72a4ed398a2dddbf6dbc21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 1\n",
      "\r",
      "Training loss: 4.616461138725281\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 2', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 2\n",
      "\r",
      "Training loss: 0.7418627934753895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 3', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 3\n",
      "\r",
      "Training loss: 0.5312441710308194\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 4', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 4\n",
      "\r",
      "Training loss: 0.3824864894151688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 5', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 5\n",
      "\r",
      "Training loss: 0.27958077045530083\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 6', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 6\n",
      "\r",
      "Training loss: 0.20749787778034806\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 7', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 7\n",
      "\r",
      "Training loss: 0.1514088394846767\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 8', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 8\n",
      "\r",
      "Training loss: 0.10168601999618113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 9', max=500.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 9\n",
      "\r",
      "Training loss: 0.08478897425206378\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 10', max=500.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 10\n",
      "\r",
      "Training loss: 0.0695862281427253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    \n",
    "pega_copyback_model = pega_copyback_model.to(torch_device)   \n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    pega_copyback_model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for b in progress_bar:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        b = tuple(x.to(torch_device) for x in b)\n",
    "        \n",
    "        inputs = {'input_ids':      b[0],\n",
    "                  'attention_mask': b[1],\n",
    "                  'labels':         b[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = pega_copyback_model(**inputs)\n",
    "#         vocab_size =  outputs['logits'].shape[2]\n",
    "    \n",
    "        loss = outputs[0]#criterion(outputs.view(-1,vocab_size),b[2].view(-1))\n",
    "\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "#         torch.nn.utils.clip_grad_norm_(pega_copyback_model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(b))})\n",
    "         \n",
    "        \n",
    "#     torch.save(pega_copyback_model.state_dict(), f'data/finetuned_pega_copyback_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pega_copyback_model.eval()\n",
    "\n",
    "tgt_text = tokenizer.batch_decode(pega_copyback_model.generate(train_data['input_ids'][[0],:].to(torch_device)))#, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"australian current account deficit narrows sharply in june quarter on commodity price rise torptn'tn'tn'tn'tnunk_3\"]\n"
     ]
    }
   ],
   "source": [
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence: five-time world champion michelle kwan withdrew from the #### us figure skating championships on wednesday , but will petition us skating officials for the chance to compete at the #### turin olympics .\n",
      "Sample Sentence: five-time world champion kwan withdraws from us championships but wants to compete in #### olympics in #### in bid to keep spot forunk_3\n",
      "Source Sentence: us business leaders lashed out wednesday at legislation that would penalize companies for employing illegal immigrants .\n",
      "Sample Sentence: us us business leaders blast anti-immigrant bill as unfair to us firms with bc-as-as-me-as-gen us-as-unk_3\n",
      "Source Sentence: general motors corp. said wednesday its us sales fell ##.# percent in december and four percent in #### with the biggest losses coming from passenger car sales .\n",
      "Sample Sentence: gm us sales in #### down ##.# pct on year on worst dec # pct on pct on pct slide in passenger car salesunk_3\n",
      "Source Sentence: several thousand people gathered on wednesday evening on the main square in zagreb for a public draw and an open air party to celebrate the croatian capital 's second chance to host the women 's slalom world cup .\n",
      "Sample Sentence: zagreb holds grand draw to host women's slalom world cup in auckland gets chance to host in auckland in auckunk_3\n",
      "@@@@@@@@@@@@@@@@@@@ Rogue @@@@@@@@@@@@@@@@@@@@@@@\n",
      "\n",
      "{'rouge1': AggregateScore(low=Score(precision=0.1859742191445004, recall=0.5474118957431457, fmeasure=0.27514053101747854), mid=Score(precision=0.20478191150195107, recall=0.5951141774891775, fmeasure=0.30036291229165346), high=Score(precision=0.22527396905317756, recall=0.6401014520202024, fmeasure=0.3270094193127252)), 'rouge2': AggregateScore(low=Score(precision=0.07769904418689141, recall=0.2479827922077922, fmeasure=0.11624590719742224), mid=Score(precision=0.09410831150248972, recall=0.2982703823953823, fmeasure=0.14065592068419586), high=Score(precision=0.11217930283027355, recall=0.34820869408369415, fmeasure=0.1650482192340151)), 'rougeL': AggregateScore(low=Score(precision=0.16707876978154643, recall=0.4964344877344878, fmeasure=0.24742353698864483), mid=Score(precision=0.18632737219538512, recall=0.5432817460317463, fmeasure=0.27363065703547584), high=Score(precision=0.20484673924815738, recall=0.5849489357864359, fmeasure=0.29826222390279367)), 'rougeLsum': AggregateScore(low=Score(precision=0.16835721228113507, recall=0.49598859126984124, fmeasure=0.24792666046059336), mid=Score(precision=0.18616825401092443, recall=0.5430245310245311, fmeasure=0.2733069897124662), high=Score(precision=0.20784993509610014, recall=0.5900640963203462, fmeasure=0.30171198858150455))}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "pega_copyback_model.eval()\n",
    "\n",
    "\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "src_text = dataset['validation']['document'][0:100]\n",
    "target_text = dataset['validation']['summary'][0:100]\n",
    "\n",
    "val_data = tokenizer.prepare_seq2seq_batch(src_text, target_text, return_tensors=\"pt\", truncation=\"only_first\", padding=\"longest\", max_length=64)\n",
    "\n",
    "input_ids_val = val_data['input_ids']\n",
    "attention_masks_val = val_data['attention_mask']\n",
    "labels_val = val_data['labels']\n",
    "\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "batch_size = 1\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size)\n",
    "\n",
    "\n",
    "shouldPrint = 0\n",
    "toPrint = 4\n",
    "with torch.no_grad():\n",
    "    for b in dataloader_val:        \n",
    "        b = tuple(x.to(torch_device) for x in b)\n",
    "        \n",
    "        inputs = {'input_ids':      b[0],\n",
    "                  'attention_mask': b[1],\n",
    "                  'labels':         b[2],\n",
    "                 }       \n",
    "        gen = tokenizer.batch_decode(pega_copyback_model.generate(inputs['input_ids']), skip_special_tokens=True)\n",
    "        if shouldPrint < toPrint:\n",
    "            print(\"Source Sentence:\", src_text[shouldPrint])\n",
    "            print(\"Sample Sentence:\", gen[0])\n",
    "            shouldPrint += 1 \n",
    "        ref = tokenizer.batch_decode(inputs['labels'])\n",
    "        metric.add_batch(predictions=gen, references=ref)\n",
    "\n",
    "\n",
    "print(\"@@@@@@@@@@@@@@@@@@@ Rogue @@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "print()\n",
    "        \n",
    "print(metric.compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
